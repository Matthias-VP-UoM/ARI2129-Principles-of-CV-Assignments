{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642da6b7-e447-451e-b6a0-a64cce2b2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.3.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.18.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.3.1-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.3.1-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "Using cached torchvision-0.18.1-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "Using cached torchaudio-2.3.1-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: intel-openmp, mkl, torch, torchvision, torchaudio\n",
      "Successfully installed intel-openmp-2021.4.0 mkl-2021.4.0 torch-2.3.1 torchaudio-2.3.1 torchvision-0.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4e6872-3e0b-4034-876b-6675e70ce695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570b154b-3b61-40a4-a92c-f5d6b0c0708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitpython in c:\\users\\mvass\\anaconda3\\lib\\site-packages (3.1.37)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from gitpython) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython) (4.0.0)\n",
      "Repository already cloned.\n",
      "Repository cloned successfully.\n"
     ]
    }
   ],
   "source": [
    "# Packages Install\n",
    "!pip install gitpython\n",
    "\n",
    "# Cloning repository\n",
    "repo_url = 'https://github.com/dylanseychell/COTSDataset.git'\n",
    "repo_dir = 'COTSDataset'  # Directory to clone the repository into\n",
    "\n",
    "# Checking if the repository directory already exists\n",
    "if not os.path.exists(repo_dir):\n",
    "    # Cloning repository\n",
    "    git.Repo.clone_from(repo_url, repo_dir)\n",
    "    print(\"Repository cloned successfully.\")\n",
    "else:\n",
    "    print(\"Repository already cloned.\")\n",
    "\n",
    "# Defining paths\n",
    "part1_single_objects = os.path.join(repo_dir, \"Part 1 - Single Objects\")\n",
    "part2_multiple_objects = os.path.join(repo_dir, \"Part 2 - Multiple Objects\")\n",
    "part3_complex_background = os.path.join(repo_dir, \"Part 3 - Complex Background\")\n",
    "\n",
    "print(\"Repository cloned successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5946613f-255d-45d6-8146-6edab93da588",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(pytorch_augmented_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m image_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m----> 6\u001b[0m random_images \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(image_files, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m pytorch_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomRotation(\u001b[38;5;241m40\u001b[39m),\n\u001b[0;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[0;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m256\u001b[39m, scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)),\n\u001b[0;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mColorJitter(brightness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, contrast\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, saturation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     14\u001b[0m ])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\random.py:456\u001b[0m, in \u001b[0;36mRandom.sample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    454\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    457\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[0;32m    458\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "image_path = 'COTSDATASET'\n",
    "pytorch_augmented_path = 'CVPICS_PYTORCH'\n",
    "os.makedirs(pytorch_augmented_path, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(image_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "random_images = random.sample(image_files, 5)\n",
    "print(f\"Selected images: {random_images}\")\n",
    "\n",
    "pytorch_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "])\n",
    "\n",
    "for image_file in random_images:\n",
    "    img = Image.open(os.path.join(image_path, image_file))\n",
    "    for i in range(3):\n",
    "        augmented_img = pytorch_transform(img)\n",
    "        save_path = os.path.join(pytorch_augmented_path, f'pytorch_aug_{os.path.splitext(image_file)[0]}_{i}.jpeg')\n",
    "        augmented_img.save(save_path)\n",
    "\n",
    "print(\"saved to\", pytorch_augmented_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cade193-3702-45b6-a3cf-aa3124352bd9",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Selected images: ['elephant_depth8.png', 'tajin_depth8_nofill.png', 'painting_depth8.png', 'cmt_mug_depth8_nofill.png', 'shampoo_colour.jpeg']\r\n",
    "saved to C:/Users/jacob/Desktop/CVPICS_PYTORCH\r\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Matthias Vassallo Pulis"
   },
   {
    "name": "Jacob Zammit"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
